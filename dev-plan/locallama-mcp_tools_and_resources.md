# ğŸ“˜ LocalLlama-MCP Server - Tool & Resource Documentation

## **Overview**
The `LocalLlama-MCP` server provides a structured way for **Cline/Roo Code/Claude Desktop** to interact with **decision-making models, retrieve existing code, and manage coding tasks efficiently**. It integrates **Retriv** for intelligent code searching and **a custom TypeScript-based decision engine** inspired by [HazyResearch/minions](https://github.com/HazyResearch/minions).

### **ğŸ”¹ Key Features:**
- **Smart Task Routing** â†’ Prioritizes existing code, then routes tasks to local/free/paid LLMs.
- **Full Job Tracking** â†’ Active jobs, progress updates, and task history.
- **Real-time Indexing Feedback** â†’ ETAs and progress bars for Retriv.
- **Token & Cost Management** â†’ Estimates costs before execution, tracks usage.

---

## **ğŸ“Œ Tools (setupToolHandlers.ts)**
These tools allow `Cline/Roo Code/Claude Desktop` to **execute coding tasks, retrieve information, and monitor execution progress**.

### **1ï¸âƒ£ route_task**  
ğŸš€ **Routes a coding task to a decision engine, manages execution, and returns results**.

**ğŸ”„ Steps:**
1. **Retriv searches for relevant code** in existing repositories.
2. **Decision engine assigns the task** (Local â†’ Free API â†’ Paid API).
3. **A new job is created** in `locallama://jobs/active`.
4. **Progress is tracked** via `locallama://jobs/progress/{jobId}`.
5. **Returns DIFF or new file** and stores in Retriv.

**ğŸ“¥ Input Schema:**
```json
{
  "task": "Refactor authentication module",
  "files_affected": ["auth.js"],
  "context_length": 4000,
  "expected_output_length": 500,
  "complexity": 0.7,
  "priority": "cost",
  "preemptive": false
}
```

**ğŸ“¤ Output Example:**
```json
{
  "job_id": "job_abc123",
  "status": "Queued",
  "eta": "3 minutes"
}
```

---

### **2ï¸âƒ£ get_cost_estimate**  
ğŸ’° **Estimates token and dollar cost before execution**.

**ğŸ“¥ Input Schema:**
```json
{
  "context_length": 4000,
  "expected_output_length": 500,
  "complexity": 0.7
}
```

**ğŸ“¤ Output Example:**
```json
{
  "local_model": "$0 (Free)",
  "openrouter_free": "$0 (Limited)",
  "openrouter_paid": "$0.10"
}
```

---

### **3ï¸âƒ£ get_active_jobs**  
ğŸ–¥ï¸ **Lists currently running jobs, including progress tracking.**

**ğŸ“¤ Output Example:**
```json
{
  "jobs": [
    {
      "id": "job_abc123",
      "task": "Refactor authentication module",
      "status": "In Progress",
      "progress": "65%",
      "estimated_time_remaining": "4 minutes"
    },
    {
      "id": "job_def456",
      "task": "Implement caching layer",
      "status": "Queued",
      "progress": "Pending",
      "estimated_time_remaining": "N/A"
    }
  ]
}
```

---

### **4ï¸âƒ£ get_job_progress/{jobId}**  
ğŸ“Š **Fetches progress of a specific job.**

**ğŸ“¤ Output Example:**
```json
{
  "jobId": "job_abc123",
  "status": "In Progress",
  "progress": "65%",
  "estimated_time_remaining": "4 minutes"
}
```

---

### **5ï¸âƒ£ cancel_job/{jobId}**  
â¹ **Cancels a running job.**

**ğŸ“¤ Output Example:**
```json
{
  "jobId": "job_abc123",
  "status": "Cancelled"
}
```

---

### **6ï¸âƒ£ get_free_models**  
ğŸ†“ **Fetches free models, ranked by performance.**

**ğŸ“¤ Output Example:**
```json
{
  "models": [
    { "name": "Mistral-7B", "accuracy": "85%", "speed": "Fast" },
    { "name": "Llama-2-13B", "accuracy": "80%", "speed": "Medium" }
  ]
}
```

---

### **7ï¸âƒ£ benchmark_task**  
ğŸ“ˆ **Benchmarks a task across different models.**

**ğŸ“¤ Output Example:**
```json
{
  "local": { "speed": "2 sec", "cost": "$0", "accuracy": "85%" },
  "openrouter_free": { "speed": "5 sec", "cost": "$0", "accuracy": "82%" },
  "openrouter_paid": { "speed": "1 sec", "cost": "$0.10", "accuracy": "90%" }
}
```

---

## **ğŸ”¹ Resources (setupResourceHandlers.ts)**

### **1ï¸âƒ£ locallama://jobs/active**  
ğŸ” **Lists all active jobs.**

---

### **2ï¸âƒ£ locallama://jobs/progress/{jobId}**  
ğŸ“Š **Real-time progress tracking for jobs.**

---

### **3ï¸âƒ£ locallama://jobs/history**  
ğŸ“œ **Lists completed jobs and results.**

---

### **4ï¸âƒ£ locallama://retriv/index-status**  
â³ **Displays current indexing progress.**

**ğŸ“¤ Output Example:**
```json
{
  "progress": "40%",
  "estimated_time_remaining": "5 minutes"
}
```

---

### **5ï¸âƒ£ locallama://minions/jobs**  
ğŸ¤– **Shows pending & running jobs in the decision engine.**

---

## **ğŸ› ï¸ Next Steps**
âœ… Implement missing API endpoints for `cancel_job` and progress tracking.
âœ… Ensure Retriv provides real-time indexing updates.
âœ… Test cost estimation accuracy before execution.
âœ… Optimize task queue handling and ensure Claude provides user-friendly VS Code feedback.

---

## **ğŸ” Summary**
This documentation ensures `LocalLlama-MCP` **fully supports intelligent task routing, cost tracking, and real-time job progress monitoring**. Claude (via Roo Code/Cline.Bot/Claude Desktop) will be able to **query, execute, and monitor tasks efficiently** while keeping the user informed in VS Code.

